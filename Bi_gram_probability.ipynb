{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-gram_probability.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1eY7q2crZl",
        "outputId": "17fe859f-f554-4842-ba36-cfc6c1d111c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQbEu-YiKOev",
        "outputId": "c5f8f083-db46-4f70-cae9-ec5113690e90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "  \n",
        "#brown corpora\n",
        "corpus=brown.words()\n",
        "\n",
        "#calculate frequency of words\n",
        "word_freq = nltk.FreqDist(w.lower() for w in corpus)\n",
        "\n",
        "\n",
        "bigram_corpus=[]\n",
        "previous='#empty'\n",
        "\n",
        "#recreate corpus\n",
        "for word in corpus:\n",
        "  if previous in ['#empty','.','?','!']:\n",
        "    bigram_corpus.append('#start_end#')\n",
        "  if word_freq[word]==1:\n",
        "    bigram_corpus.append('#oov#')\n",
        "  else:\n",
        "    bigram_corpus.append(word)\n",
        "  previous=word\n",
        "\n",
        "#update frequency\n",
        "new_word_freq = nltk.FreqDist(w.lower() for w in bigram_corpus)\n",
        "\n",
        "#bigrams corpora\n",
        "bigrams = nltk.bigrams(w.lower() for w in bigram_corpus)\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)\n",
        "total_words=len(bigram_corpus)\n",
        "\n",
        "#define function unigram probability\n",
        "def unigram_probability(word):\n",
        "  if word in bigram_corpus:\n",
        "    return new_word_freq[word]/total_words#prob=freq/total words count\n",
        "  else:\n",
        "    return new_word_freq['#oov#']/total_words\n",
        "\n",
        "#define function bigram probability\n",
        "def bigram_probability(word2,word1):\n",
        "  if word2 not in cfd[word1]:\n",
        "    return unigram_probability(word1)\n",
        "  else:\n",
        "    bigram_frequency = cfd[word1][word2]#prob=freq(n,n-1)/freq(n-1)\n",
        "    unigram_frequency = new_word_freq[word1]\n",
        "    return (bigram_frequency/unigram_frequency)\n",
        "\n",
        "#define function calculate probabilty\n",
        "def calculate_probability(sentence):\n",
        "  x=word_tokenize(sentence)#tokenize sentences\n",
        "  x.insert(0,\"#start_end#\")\n",
        "  x.append(\"#start_end#\")\n",
        "  \n",
        "  for i in range(len(x)):\n",
        "    if x[i] not in bigram_corpus:#replace out of vocabulary words with #oov#\n",
        "      x[i]=\"#oov#\"\n",
        "    x[i]=x[i].lower()#lower words\n",
        "  print(\"Tokenized sentence:\",x)\n",
        "  y=[]\n",
        "  for i in range(1,len(x)):\n",
        "    y.append((x[i],x[i-1]))#create bi-grams\n",
        "  print(\"Bigrams:\",y)\n",
        "  proba=1\n",
        "  for word in y:\n",
        "    res=bigram_probability(word[1],word[0])\n",
        "    print(\"(\",word[1],\",\",word[0],\"):\",res)\n",
        "    proba=proba*res\n",
        "  \n",
        "  return proba"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AUpZvhCKOPE",
        "outputId": "4a463648-6847-4671-b7b4-a4275ab71684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence=input(\"Enter sentence: \")\n",
        "result=calculate_probability(sentence)\n",
        "print(\"Probability of this sentence using bigram model is:\",result)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter sentence: this is one of the best experience.\n",
            "Tokenized sentence: ['#start_end#', 'this', 'is', 'one', 'of', 'the', 'best', 'experience', '.', '#start_end#']\n",
            "Bigrams: [('this', '#start_end#'), ('is', 'this'), ('one', 'is'), ('of', 'one'), ('the', 'of'), ('best', 'the'), ('experience', 'best'), ('.', 'experience'), ('#start_end#', '.')]\n",
            "( #start_end# , this ): 0.004228209926308341\n",
            "( this , is ): 0.004352557127312296\n",
            "( is , one ): 0.013669501822600244\n",
            "( one , of ): 0.0026914204108535646\n",
            "( of , the ): 0.057502833188283954\n",
            "( the , best ): 0.008547008547008548\n",
            "( best , experience ): 0.00022681942461829003\n",
            "( experience , . ): 0.0405530120551237\n",
            "( . , #start_end# ): 0.045721372060284655\n",
            "Probability of this sentence using bigram model is: 1.3994624416756706e-19\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}