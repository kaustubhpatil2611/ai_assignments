{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bigramprobabilty.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMD8C/bGJ8AglnEmAmK2QvN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaustubhpatil2611/ai_assignments/blob/master/Bigramprobabilty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld1eY7q2crZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b94d68-22ba-47a8-d66f-325d9fb25a6f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQbEu-YiKOev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "335410ac-d95f-4ba0-8d04-09aa958b1181"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "  \n",
        "#brown corpora\n",
        "corpus=brown.words()\n",
        "\n",
        "#calculate frequency of words\n",
        "word_freq = nltk.FreqDist(w.lower() for w in corpus)\n",
        "\n",
        "\n",
        "bigram_corpus=[]\n",
        "previous='#empty'\n",
        "\n",
        "#recreate corpus\n",
        "for word in corpus:\n",
        "  if previous in ['#empty','.','?','!']:\n",
        "    bigram_corpus.append('*start_end*')\n",
        "  if word_freq[word]==1:\n",
        "    bigram_corpus.append('*oov*')\n",
        "  else:\n",
        "    bigram_corpus.append(word)\n",
        "  previous=word\n",
        "\n",
        "#update frequency\n",
        "new_word_freq = nltk.FreqDist(w.lower() for w in bigram_corpus)\n",
        "\n",
        "#bigrams corpora\n",
        "bigrams = nltk.bigrams(w.lower() for w in bigram_corpus)\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)\n",
        "total_words=len(bigram_corpus)\n",
        "\n",
        "#define function unigram probability\n",
        "def unigram_probability(word):\n",
        "  if word in bigram_corpus:\n",
        "    return new_word_freq[word]/total_words#prob=freq/total words count\n",
        "  else:\n",
        "    return new_word_freq['#oov#']/total_words\n",
        "\n",
        "#define function bigram probability\n",
        "def bigram_probability(word1,word2):\n",
        "  if word2 not in cfd[word1]:\n",
        "    return unigram_probability(word2)\n",
        "  else:\n",
        "    bigram_frequency = cfd[word1][word2]#prob=freq(Wn-1Wn)/freq(Wn-1)\n",
        "    unigram_frequency = new_word_freq[word1]\n",
        "    return (bigram_frequency/unigram_frequency)\n",
        "\n",
        "#define function calculate probabilty\n",
        "def calculate_probability(sentence):\n",
        "  x=word_tokenize(sentence)#tokenize sentences\n",
        "  x.insert(0,\"*start_end*\")\n",
        "  x.append(\"*start_end*\")\n",
        "  \n",
        "  for i in range(len(x)):\n",
        "    if x[i] not in bigram_corpus:#replace out of vocabulary words with #oov#\n",
        "      x[i]=\"*oov*\"\n",
        "    x[i]=x[i].lower()#lower words\n",
        "  print(\"Tokenized sentence:\",x)\n",
        "  y=[]\n",
        "  for i in range(1,len(x)):\n",
        "    y.append((x[i-1],x[i]))#create bi-grams\n",
        "  print(\"Bigrams:\",y)\n",
        "  proba=1\n",
        "  for word in y:\n",
        "    res=bigram_probability(word[0],word[1])\n",
        "    print(\"(\",word[0],\",\",word[1],\"):\",res)\n",
        "    proba=proba*res\n",
        "  \n",
        "  return proba"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh6z1kO3lZfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a6dc85-a68b-48b8-bd97-395fe38aa3e1"
      },
      "source": [
        "#bigram model\n",
        "sentence=input(\"Enter sentence: \")\n",
        "result=calculate_probability(sentence)\n",
        "print(\"Probability of this sentence using bigram model is:\",result)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter sentence: i like artificial intelligence very much\n",
            "Tokenized sentence: ['*start_end*', 'i', 'like', 'artificial', 'intelligence', 'very', 'much', '*start_end*']\n",
            "Bigrams: [('*start_end*', 'i'), ('i', 'like'), ('like', 'artificial'), ('artificial', 'intelligence'), ('intelligence', 'very'), ('very', 'much'), ('much', '*start_end*')]\n",
            "( *start_end* , i ): 0.02390581468500045\n",
            "( i , like ): 0.005034856700232378\n",
            "( like , artificial ): 1.3970761661271487e-05\n",
            "( artificial , intelligence ): 3.9446856455354785e-05\n",
            "( intelligence , very ): 0.0006541603695513002\n",
            "( very , much ): 0.059045226130653265\n",
            "( much , *start_end* ): 0.045721372060284655\n",
            "Probability of this sentence using bigram model is: 1.171416727755512e-19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSBwpCRI_qMg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}